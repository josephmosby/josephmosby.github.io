<!doctype html>
<html>
	<head>

		<title>josephmosby.com</title>
		<meta http-equiv="Content-Type" content="text/html" charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

		<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
		<link rel="stylesheet" href="/css/main.css" />

	</head>
	
	<body>
		
		<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
			<a class="navbar-brand" href="/">JM</a>
			<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
				<span class="navbar-toggler-icon"></span>
			</button>

			<div id="navbarCollapse" class="collapse navbar-collapse">
				<ul class="navbar-nav mr-auto">
					<li class="nav-item"><a href="/archives">Archives</a></li>
					<li class="nav-item"><a href="/about">About</a></li>
					<li class="nav-item"><a href="/presentations">Presentations</a></li>
					<li class="nav-item"><a href="/feed/atom.xml">RSS</a></li>
				</ul>
			</div>
		</nav>
		
		<main class="container " role="main">
			<div class="row">
	<div class="col-md-4 col-sm-12">
		<div class="post-fm"><time>21 Oct 2015</time>
			<a href="/tags/code">code</a>
		</div>
		<h3>nginx won't timeout, and other tails from the log files</h3>

	</div>
	<div class="col-md-8 col-sm-12">
		<p>We had yet another instance of nginx allowing requests to spin into infinity, and now I’m starting to get a little frustrated with it. I’m going to take a different tack and see if I can sniff out the point when these requests start to blow up. To do this, I’m going to use the <code class="highlighter-rouge">awk</code> tool to find any and all requests that take longer than 3000 milliseconds, which is well beyond the tolerance point for my application. Most of our requests average out in the 100-150ms range, with long running requests taking 500ms. Let’s dump some things out from the log files.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat django-www.log | awk '$33 &gt; 3000 {print NR-1 ": " $0;}' &gt; high.log
</code></pre></div></div>

<p>So what this little snippet will do is <code class="highlighter-rouge">cat</code> the entire log file out, then pipe the output through an <code class="highlighter-rouge">awk</code> command. The syntax of <code class="highlighter-rouge">awk</code> breaks the file up based on delimiters (I think space is the default), which you can then access by number. <code class="highlighter-rouge">$33</code>, in this case, is the 33rd character, which is our millisecond mark. If it’s greater than 3000, I print the line number, the line itself, then dump all that out to a file called <code class="highlighter-rouge">high.log</code>.</p>

<p>Onward.</p>

<p>I don’t know why this stuck out to me, but I gave the address space usage and the rss usage of my uwsgi threads a second look this time. Here’s what they look like under normal traffic for a small sample.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{address space usage: 3510480896 bytes/3347MB} {rss usage: 84557824 bytes/80MB} [pid: 5542|app: 0|req: 394/98579] 
{address space usage: 3510046720 bytes/3347MB} {rss usage: 101765120 bytes/97MB} [pid: 5875|app: 0|req: 387/98580] 
{address space usage: 3510497280 bytes/3347MB} {rss usage: 106389504 bytes/101MB} [pid: 6922|app: 0|req: 70/98581] 
{address space usage: 3510497280 bytes/3347MB} {rss usage: 106500096 bytes/101MB} [pid: 6922|app: 0|req: 71/98582] 
{address space usage: 3521171456 bytes/3358MB} {rss usage: 135237632 bytes/128MB} [pid: 4706|app: 0|req: 660/98583] 
{address space usage: 3510046720 bytes/3347MB} {rss usage: 101765120 bytes/97MB} [pid: 5875|app: 0|req: 388/98584] 
{address space usage: 3510480896 bytes/3347MB} {rss usage: 84557824 bytes/80MB} [pid: 5542|app: 0|req: 395/98585] 
</code></pre></div></div>

<p>The address space usage hovers at about 3.3GB, but the rss usage averages out around 100MB under normal traffic. Here’s what it looks like when we spike:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{address space usage: 3518722048 bytes/3355MB} {rss usage: 148234240 bytes/141MB} [pid: 28562|app: 0|req: 4128/36509]
{address space usage: 3537321984 bytes/3373MB} {rss usage: 153063424 bytes/145MB} [pid: 28827|app: 0|req: 4137/36512]
{address space usage: 3518103552 bytes/3355MB} {rss usage: 125833216 bytes/120MB}
{address space usage: 3537321984 bytes/3373MB} {rss usage: 153255936 bytes/146MB} [pid: 28827|app: 0|req: 4138/36517]
{address space usage: 3518722048 bytes/3355MB} {rss usage: 148992000 bytes/142MB} [pid: 28562|app: 0|req: 4137/36523]
{address space usage: 3518722048 bytes/3355MB} {rss usage: 148992000 bytes/142MB} [pid: 28562|app: 0|req: 4137/36525]
</code></pre></div></div>

<p>Our address space usage is the same, but our rss usage is pushing over 142MB for almost every request. I want to dig more into this.</p>

<p>I stumbled upon this discussion thread on the <code class="highlighter-rouge">uwsgi</code> issues page from someone experiencing the same sort of performance degradation with almost the same sort of configuration that we have: <code class="highlighter-rouge">uwsgi</code>, <code class="highlighter-rouge">supervisord</code>, Django. The solution suggested here is to add <code class="highlighter-rouge">die-on-term=True</code> to our <code class="highlighter-rouge">uwsgi</code> config, but I want to look into that a little more before I just start adding things to our <code class="highlighter-rouge">uwsgi</code> config. (the issue thread is <a href="https://github.com/unbit/uwsgi/issues/296">here</a>)</p>

<p>The issue is distilled <a href="http://uwsgi-docs.readthedocs.org/en/latest/ThingsToKnow.html">here</a> (second bullet). Before uWSGI 2.1, sending the <code class="highlighter-rouge">SIGTERM</code> signal to <code class="highlighter-rouge">uwsgi</code> means “brutally reload the stack”, which is not convention. <code class="highlighter-rouge">SIGINT</code> or <code class="highlighter-rouge">SIGQUIT</code> has the same behavior in uWSGI that <code class="highlighter-rouge">SIGTERM</code> has in other applications. Searching for <code class="highlighter-rouge">supervisord SIGTERM</code> yielded this StackOverflow answer:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>supervisord will emit a SIGTERM signal when a stop is requested. Your child can very probably catch and process this signal (the stopsignal configuration can change the signal sent).

http://stackoverflow.com/a/20299217/1020642
</code></pre></div></div>

<p>But my child CAN’T catch and process that signal. In fact, it <a href="https://github.com/unbit/uwsgi/issues/296#issuecomment-36086359">actually totally ignores it</a>. It trips over it and brutally reloads the stack if I’m running <code class="highlighter-rouge">uwsgi</code> prior to 2.1.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ uwsgi --version
2.0.9
</code></pre></div></div>

<p>So to fix this bug, we either need to upgrade <code class="highlighter-rouge">uwsgi</code>, or send the <code class="highlighter-rouge">die-on-term</code> option, which will correct this behavior. Adding the <code class="highlighter-rouge">die-on-term</code> directive is the quicker and less potentially problematic version. This will go in our <code class="highlighter-rouge">uwsgi.ini</code> file:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[uwsgi]
... stuff ...
processes=4
threads=256
harakiri=20
max-requests=5000
die-on-term=True # yay
... stuff ...
</code></pre></div></div>

<p>And now we’ll reload and give it a shot.</p>

	</div>
</div>
		</main>

		<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
		<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
		<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
	</body>

</html>
